#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ GGUF –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace
"""

import os
from pathlib import Path
from huggingface_hub import HfApi, create_repo

def create_model_card():
    """–°–æ–∑–¥–∞–µ—Ç README.md –¥–ª—è –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace"""
    return """---
license: apache-2.0
base_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
tags:
  - gguf
  - russian
  - legal
  - investigator
  - tinyLlama
  - quantized
language:
  - ru
pipeline_tag: text-generation
---

# –°–õ–ï–î–û–í–ê–¢–ï–õ–¨ - –°—Ñ–µ—Ä–∞ 047 (M4 Overnight) - GGUF

–≠—Ç–æ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ [nativemind/sphere_047_m4_overnight](https://huggingface.co/nativemind/sphere_047_m4_overnight) –≤ —Ñ–æ—Ä–º–∞—Ç–µ GGUF, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.

## üìã –û –º–æ–¥–µ–ª–∏

**–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å:** TinyLlama/TinyLlama-1.1B-Chat-v1.0  
**–û–±—É—á–µ–Ω–æ –Ω–∞:** M4 MacBook Pro –∑–∞ ~2 —á–∞—Å–∞  
**–ú–µ—Ç–æ–¥:** LoRA (rank=8)  
**–î–∞—Ç–∞—Å–µ—Ç:** –†–µ–∞–ª—å–Ω–æ–µ —É–≥–æ–ª–æ–≤–Ω–æ–µ –¥–µ–ª–æ + Alpaca + Kene  
**–§–æ—Ä–º–∞—Ç:** GGUF (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ PyTorch + LoRA)

## üì¶ –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏

| –§–∞–π–ª | –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è | –†–∞–∑–º–µ—Ä | –û–ø–∏—Å–∞–Ω–∏–µ |
|------|-------------|--------|----------|
| `sphere_047_m4_overnight.gguf` | F16 | ~2.2 GB | –ü–æ–ª–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å |
| `sphere_047_m4_overnight-q4_0.gguf` | Q4_0 | ~630 MB | 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è |
| `sphere_047_m4_overnight-q4_k_m.gguf` | Q4_K_M | ~650 MB | 4-bit K-–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (—Å—Ä–µ–¥–Ω—è—è) |
| `sphere_047_m4_overnight-q5_k_m.gguf` | Q5_K_M | ~750 MB | 5-bit K-–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (—Å—Ä–µ–¥–Ω—è—è) |
| `sphere_047_m4_overnight-q8_0.gguf` | Q8_0 | ~1.2 GB | 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è |

## üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### llama.cpp

```bash
# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å
huggingface-cli download nativemind/sphere_047_m4_overnight-gguf sphere_047_m4_overnight-q4_k_m.gguf

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ inference
./llama.cpp/build/bin/llama-cli -m sphere_047_m4_overnight-q4_k_m.gguf -p "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç..." -n 512
```

### Flutter Llama Plugin

```dart
import 'package:flutter_llama/flutter_llama.dart';

final llama = FlutterLlama();

// –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å
await llama.loadModel(
  modelPath: 'path/to/sphere_047_m4_overnight-q4_k_m.gguf',
  config: LlamaConfig(
    contextSize: 2048,
    numThreads: 4,
  ),
);

// –ì–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ —Ç–µ–∫—Å—Ç
final response = await llama.generateText(
  prompt: '–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç: ...',
  maxTokens: 512,
);

print(response);
```

### Python (llama-cpp-python)

```python
from llama_cpp import Llama

llm = Llama(
    model_path="sphere_047_m4_overnight-q4_k_m.gguf",
    n_ctx=2048,
    n_threads=4,
)

output = llm(
    "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç: ...",
    max_tokens=512,
    temperature=0.7,
)

print(output['choices'][0]['text'])
```

## üì± –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º

- **–ú–æ–±–∏–ª—å–Ω—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (iOS/Android):** Q4_0 –∏–ª–∏ Q4_K_M
- **–ù–æ—É—Ç–±—É–∫–∏/Desktop:** Q5_K_M –∏–ª–∏ Q8_0
- **–°–µ—Ä–≤–µ—Ä—ã:** F16 (–ø–æ–ª–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)

## üéØ –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–º–ø—Ç–æ–≤

```
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã...

–°–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –¥–µ–ª–∞...

–û–ø—Ä–µ–¥–µ–ª–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –ø–æ–∫–∞–∑–∞–Ω–∏—è—Ö —Å–≤–∏–¥–µ—Ç–µ–ª–µ–π...
```

## ‚öñÔ∏è –õ–∏—Ü–µ–Ω–∑–∏—è

Apache 2.0

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏

- –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: [TinyLlama Team](https://github.com/jzhang38/TinyLlama)
- GGUF –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: [llama.cpp](https://github.com/ggerganov/llama.cpp)

**‚öñÔ∏è –ò—Å—Ç–∏–Ω–∞ –≤–æ—Å—Ç–æ—Ä–∂–µ—Å—Ç–≤—É–µ—Ç! üïâÔ∏è**
"""

def main():
    print("=== –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace ===\n")
    
    repo_id = "nativemind/sphere_047_m4_overnight-gguf"
    gguf_dir = Path("models_temp/gguf")
    
    if not gguf_dir.exists():
        print("‚úó –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å GGUF —Ñ–∞–π–ª–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print("  –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python convert_to_gguf.py")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
    gguf_files = list(gguf_dir.glob("*.gguf"))
    if not gguf_files:
        print("‚úó GGUF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        return
    
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(gguf_files)} GGUF —Ñ–∞–π–ª–æ–≤:")
    for f in gguf_files:
        size_mb = f.stat().st_size / (1024 * 1024)
        print(f"  - {f.name} ({size_mb:.1f} MB)")
    
    print("\n1. –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ HuggingFace...")
    
    try:
        api = HfApi()
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (–µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
        try:
            create_repo(
                repo_id=repo_id,
                repo_type="model",
                exist_ok=True,
                private=False,
            )
            print(f"‚úì –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å–æ–∑–¥–∞–Ω: https://huggingface.co/{repo_id}")
        except Exception as e:
            print(f"–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –æ—à–∏–±–∫–∞: {e}")
        
        print("\n2. –ó–∞–≥—Ä—É–∑–∫–∞ README.md...")
        readme_content = create_model_card()
        readme_path = gguf_dir / "README.md"
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(readme_content)
        
        api.upload_file(
            path_or_fileobj=str(readme_path),
            path_in_repo="README.md",
            repo_id=repo_id,
            repo_type="model",
        )
        print("‚úì README.md –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        print("\n3. –ó–∞–≥—Ä—É–∑–∫–∞ GGUF —Ñ–∞–π–ª–æ–≤...")
        for gguf_file in gguf_files:
            print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ {gguf_file.name}...")
            size_mb = gguf_file.stat().st_size / (1024 * 1024)
            print(f"  –†–∞–∑–º–µ—Ä: {size_mb:.1f} MB")
            
            api.upload_file(
                path_or_fileobj=str(gguf_file),
                path_in_repo=gguf_file.name,
                repo_id=repo_id,
                repo_type="model",
            )
            print(f"‚úì {gguf_file.name} –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        print("\n" + "="*60)
        print("‚úì –í—Å–µ —Ñ–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        print(f"\nüîó –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ –∞–¥—Ä–µ—Å—É:")
        print(f"   https://huggingface.co/{repo_id}")
        print("="*60)
        
    except Exception as e:
        print(f"\n‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
        print("\n–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –≤—ã –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã:")
        print("  huggingface-cli login")

if __name__ == "__main__":
    main()

